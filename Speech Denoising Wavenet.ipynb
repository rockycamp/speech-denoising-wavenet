{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!THEANO_FLAGS=optimizer=fast_compile,device=cuda*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "/home/david/tensorflow/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "import optparse\n",
    "import json\n",
    "import os\n",
    "import models\n",
    "import datasets\n",
    "import util\n",
    "import denoise\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_system_settings():\n",
    "    sys.setrecursionlimit(50000)\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "set_system_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def load_config(config_filepath):\n",
    "    try:\n",
    "        config_file = open(config_filepath, 'r')\n",
    "    except IOError:\n",
    "        logging.error('No readable config file at path: ' + config_filepath)\n",
    "        exit()\n",
    "    else:\n",
    "        with config_file:\n",
    "            return json.load(config_file)\n",
    "\n",
    "def get_valid_output_folder_path(outputs_folder_path):\n",
    "    j = 1\n",
    "    while True:\n",
    "        output_folder_name = 'samples_%d' % j\n",
    "        output_folder_path = os.path.join(outputs_folder_path, output_folder_name)\n",
    "        if not os.path.isdir(output_folder_path):\n",
    "            os.mkdir(output_folder_path)\n",
    "            break\n",
    "        j += 1\n",
    "    return output_folder_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First replicate the inference mode. The example shows the following parameters passed to the program:\n",
    "\n",
    "THEANO_FLAGS=optimizer=fast_compile,device=gpu \n",
    "python main.py \n",
    "--mode inference \n",
    "--config sessions/001/config.json \n",
    "--noisy_input_path data/NSDTSEA/noisy_testset_wav \n",
    "--clean_input_path data/NSDTSEA/clean_testset_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "class Params():\n",
    "    def __init__(self):\n",
    "        self.batch_size=None\n",
    "        self.config='sessions/001/config.json'\n",
    "        self.mode='inference'\n",
    "        self.load_checkpoint=None #'sessions/002/checkpoints/checkpoint.00144.hdf5'\n",
    "        self.condition_value=0\n",
    "        self.batch_size=None\n",
    "        self.one_shot=False\n",
    "        #self.clean_input_path='/home/david/data/CSTR_VCTK_Corpus/clean_trainset_28spk_wav/p226_001.wav'\n",
    "        #self.noisy_input_path='/home/david/data/CSTR_VCTK_Corpus/noisy_trainset_28spk_wav/p226_001.wav'\n",
    "        #self.clean_input_path='/home/david/data/CSTR_VCTK_Corpus/clean_testset_wav/p232_104.wav'\n",
    "        #self.noisy_input_path='/home/david/data/CSTR_VCTK_Corpus/noisy_testset_wav/p232_104.wav'\n",
    "        #self.clean_input_path='/home/david/data/CSTR_VCTK_Corpus/clean_testset_28spk_wav/p232_001.wav'\n",
    "        #self.noisy_input_path='/home/david/data/CSTR_VCTK_Corpus/noisy_testset_28spk_wav/p232_001.wav'        \n",
    "        self.clean_input_path='data/CSTR_VCTK_Corpus/clean_testset_wav/p232_104.wav'\n",
    "        self.noisy_input_path='data/CSTR_VCTK_Corpus/noisy_testset_wav/p232_104.wav'        \n",
    "        self.print_model_summary=False\n",
    "        self.target_field_length=None\n",
    "\n",
    "params = Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config        \n",
    "config = load_config(params.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve params and config\n",
    "if params.batch_size is not None:\n",
    "    batch_size = int(params.batch_size)\n",
    "else:\n",
    "    batch_size = config['training']['batch_size']\n",
    "\n",
    "if params.target_field_length is not None:\n",
    "    params.target_field_length = int(params.target_field_length)\n",
    "\n",
    "if not bool(params.one_shot):\n",
    "    model = models.DenoisingWavenet(config, target_field_length=params.target_field_length,\n",
    "                                    load_checkpoint=params.load_checkpoint, \n",
    "                                    print_model_summary=params.print_model_summary)\n",
    "    print('Performing inference..')\n",
    "else:\n",
    "    print('Performing one-shot inference..')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursively search session folders to retrieve files for inference\n",
    "samples_folder_path = os.path.join(config['training']['path'], 'samples')\n",
    "output_folder_path = get_valid_output_folder_path(samples_folder_path)\n",
    "\n",
    "#If input_path is a single wav file, then set filenames to single element with wav filename\n",
    "if params.noisy_input_path.endswith('.wav'):\n",
    "    filenames = [params.noisy_input_path.rsplit('/', 1)[-1]]\n",
    "    params.noisy_input_path = params.noisy_input_path.rsplit('/', 1)[0] + '/'\n",
    "    if params.clean_input_path is not None:\n",
    "        params.clean_input_path = params.clean_input_path.rsplit('/', 1)[0] + '/'\n",
    "else:\n",
    "    if not params.noisy_input_path.endswith('/'):\n",
    "        params.noisy_input_path += '/'\n",
    "    filenames = [filename for filename in os.listdir(params.noisy_input_path) if filename.endswith('.wav')]\n",
    "\n",
    "clean_input = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples_folder_path)\n",
    "print(output_folder_path)\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom the inference\n",
    "for filename in filenames:\n",
    "    noisy_input = util.load_wav(params.noisy_input_path + filename, config['dataset']['sample_rate'])\n",
    "    if params.clean_input_path is not None:\n",
    "        if not params.clean_input_path.endswith('/'):\n",
    "            params.clean_input_path += '/'\n",
    "        clean_input = util.load_wav(params.clean_input_path + filename, config['dataset']['sample_rate'])\n",
    "\n",
    "    input = {'noisy': noisy_input, 'clean': clean_input}\n",
    "\n",
    "    output_filename_prefix = filename[0:-4] + '_'\n",
    "\n",
    "    if config['model']['condition_encoding'] == 'one_hot':\n",
    "        condition_input = util.one_hot_encode(int(params.condition_value), 29)[0]\n",
    "    else:\n",
    "        condition_input = util.binary_encode(int(params.condition_value), 29)[0]\n",
    "\n",
    "    if bool(params.one_shot):\n",
    "        if len(input['noisy']) % 2 == 0:  # If input length is even, remove one sample\n",
    "            input['noisy'] = input['noisy'][:-1]\n",
    "            if input['clean'] is not None:\n",
    "                input['clean'] = input['clean'][:-1]\n",
    "        model = models.DenoisingWavenet(config, \n",
    "                                        load_checkpoint=params.load_checkpoint, \n",
    "                                        input_length=len(input['noisy']), \n",
    "                                        print_model_summary=params.print_model_summary)\n",
    "\n",
    "    print(\"Denoising: \",filename)\n",
    "    denoise.denoise_sample(model, input, condition_input, batch_size, output_filename_prefix,\n",
    "                                        config['dataset']['sample_rate'], output_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"receptive field length: \",model.receptive_field_length, \" samples, \",model.receptive_field_length*1000/16000, \" ms\")\n",
    "print(\"target field length: \",model.target_field_length, \" samples, \",model.target_field_length*1000/16000, \" ms\")\n",
    "print(\"input length: \",model.input_length, \" samples, \", model.input_length*1000/16000,\" ms\")\n",
    "print(\"target padding: \",model.target_padding, \"samples, \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    def __init__(self):\n",
    "        self.batch_size=4\n",
    "        self.config='sessions/003/config.json'\n",
    "        self.mode='training'\n",
    "        self.load_checkpoint=None\n",
    "        self.condition_value=0\n",
    "        self.batch_size=None\n",
    "        self.one_shot=False        \n",
    "        self.clean_input_path=None\n",
    "        self.noisy_input_path=None        \n",
    "        self.print_model_summary=False\n",
    "        self.target_field_length=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load params and then config (dependency)\n",
    "params = Params()\n",
    "config = load_config(params.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new model...\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Model\n",
    "model = models.DenoisingWavenet(config, \n",
    "                                load_checkpoint=params.load_checkpoint, \n",
    "                                print_model_summary=params.print_model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receptive field length:  3067  samples,  191.6875  ms\n",
      "target field length:  1601  samples,  100.0625  ms\n",
      "input length:  4667  samples,  291.6875  ms\n",
      "target padding:  1 samples, \n",
      "dilations:  [1, 2, 4, 8, 16, 32, 64, 128, 256]\n"
     ]
    }
   ],
   "source": [
    "print(\"receptive field length: \",model.receptive_field_length, \" samples, \",model.receptive_field_length*1000/16000, \" ms\")\n",
    "print(\"target field length: \",model.target_field_length, \" samples, \",model.target_field_length*1000/16000, \" ms\")\n",
    "print(\"input length: \",model.input_length, \" samples, \", model.input_length*1000/16000,\" ms\")\n",
    "print(\"target padding: \",model.target_padding, \"samples, \")\n",
    "print(\"dilations: \", model.dilations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NSDTSEA dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/signal/signaltools.py:2365: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return y[keep]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'datasets' has no attribute 'in_memory_percentage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d4aacdd6eafc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"in memory percentage: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_memory_percentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'datasets' has no attribute 'in_memory_percentage'"
     ]
    }
   ],
   "source": [
    "def get_dataset(config, model):\n",
    "\n",
    "    if config['dataset']['type'] == 'vctk+demand':\n",
    "        return datasets.VCTKAndDEMANDDataset(config, model).load_dataset()\n",
    "    elif config['dataset']['type'] == 'nsdtsea':\n",
    "        return datasets.NSDTSEADataset(config, model).load_dataset()\n",
    "\n",
    "dataset = get_dataset(config, model)\n",
    "print(\"in memory percentage: \", dataset.in_memory_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model with  1000  training samples and  100  test samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/dev/speech-denoising-wavenet/models.py:164: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  initial_epoch=self.epoch_num)\n",
      "/home/david/dev/speech-denoising-wavenet/models.py:164: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., 1000, callbacks=[<keras.ca..., validation_steps=100, verbose=1, initial_epoch=0, validation_data=<generator..., epochs=250)`\n",
      "  initial_epoch=self.epoch_num)\n",
      "WARNING (theano.tensor.blas): We did not find a dynamic library in the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/signal/signaltools.py:2365: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return y[keep]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 827/1000 [=======================>......] - ETA: 1:32:19 - loss: 0.2275 - data_output_1_loss: 0.1137 - data_output_2_loss: 0.1137 - data_output_1_mean_absolute_error: 0.1137 - data_output_1_valid_mean_absolute_error: 0.1138 - data_output_2_mean_absolute_error: 0.1137 - data_output_2_valid_mean_absolute_error: 0.1138"
     ]
    }
   ],
   "source": [
    "# Perfrom Training\n",
    "num_train_samples = config['training']['num_train_samples']\n",
    "batch_size = config['training']['batch_size']\n",
    "steps_per_epoch = num_train_samples//batch_size\n",
    "num_test_samples = config['training']['num_test_samples']\n",
    "validation_steps = num_test_samples//batch_size\n",
    "train_set_generator = dataset.get_random_batch_generator('train')\n",
    "test_set_generator = dataset.get_random_batch_generator('test')\n",
    "\n",
    "model.fit_model(train_set_generator, steps_per_epoch, test_set_generator, validation_steps,\n",
    "                      config['training']['num_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (tensorflow)\n",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
